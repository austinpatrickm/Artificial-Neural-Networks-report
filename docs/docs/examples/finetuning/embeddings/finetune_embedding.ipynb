{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28a8b793",
      "metadata": {
        "id": "28a8b793"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/embeddings/finetune_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551753b7-6cd2-4f81-aec0-da119e4705ad",
      "metadata": {
        "id": "551753b7-6cd2-4f81-aec0-da119e4705ad"
      },
      "source": [
        "# Finetune Embeddings\n",
        "\n",
        "In this notebook, we show users how to finetune their own embedding models.\n",
        "\n",
        "We go through three main sections:\n",
        "1. Preparing the data (our `generate_qa_embedding_pairs` function makes this easy)\n",
        "2. Finetuning the model (using our `SentenceTransformersFinetuneEngine`)\n",
        "3. Evaluating the model on a validation knowledge corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99afd542-fc47-44ac-aed0-b3684108dba5",
      "metadata": {
        "id": "99afd542-fc47-44ac-aed0-b3684108dba5"
      },
      "source": [
        "## Generate Corpus\n",
        "\n",
        "First, we create the corpus of text chunks by leveraging LlamaIndex to load some financial PDFs, and parsing/chunking into plain text chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e973679e",
      "metadata": {
        "id": "e973679e"
      },
      "outputs": [],
      "source": [
        "%pip install datasets\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install llama-index-finetuning\n",
        "%pip install llama-index-readers-file\n",
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install \"transformers[torch]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb",
      "metadata": {
        "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import MetadataMode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c42620",
      "metadata": {
        "id": "73c42620"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18",
      "metadata": {
        "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18"
      },
      "outputs": [],
      "source": [
        "TRAIN_FILES = [\"./data/train_dataset.pdf\"]\n",
        "VAL_FILES = [\"./data/test_dataset.pdf\"]\n",
        "\n",
        "TRAIN_CORPUS_FPATH = \"./data/train_corpus.json\"\n",
        "VAL_CORPUS_FPATH = \"./data/val_corpus.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da871c1-9d58-467a-92fd-06ed3d94534b",
      "metadata": {
        "id": "1da871c1-9d58-467a-92fd-06ed3d94534b"
      },
      "outputs": [],
      "source": [
        "def load_corpus(files, verbose=False):\n",
        "    if verbose:\n",
        "        print(f\"Loading files {files}\")\n",
        "\n",
        "    reader = SimpleDirectoryReader(input_files=files)\n",
        "    docs = reader.load_data()\n",
        "    if verbose:\n",
        "        print(f\"Loaded {len(docs)} docs\")\n",
        "\n",
        "    parser = SentenceSplitter()\n",
        "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Parsed {len(nodes)} nodes\")\n",
        "\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53056d8b-3b4c-4364-9b07-a375aa84330b",
      "metadata": {
        "id": "53056d8b-3b4c-4364-9b07-a375aa84330b"
      },
      "source": [
        "We do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
      "metadata": {
        "id": "d3651c77-d085-4fbc-bb34-61f143ad6674"
      },
      "outputs": [],
      "source": [
        "train_nodes = load_corpus(TRAIN_FILES, verbose=True)\n",
        "val_nodes = load_corpus(VAL_FILES, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4482c48-844b-448b-9552-3f38b455645c",
      "metadata": {
        "id": "b4482c48-844b-448b-9552-3f38b455645c"
      },
      "source": [
        "### Generate synthetic queries\n",
        "\n",
        "Now, we use an LLM (gpt-3.5-turbo) to generate questions using each text chunk in the corpus as context.\n",
        "\n",
        "Each pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f",
      "metadata": {
        "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f"
      },
      "outputs": [],
      "source": [
        "from llama_index.finetuning import generate_qa_embedding_pairs\n",
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666001e2",
      "metadata": {
        "id": "666001e2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY = \"sk-\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CUSTOM_PROMPT = \"\"\"\n",
        "Context from FL Studio documentation:\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Imagine you are an FL Studio user reading the above.\n",
        "Generate {num_questions_per_chunk} questions this user might ask about this specific information to understand or use it in FL Studio.\n",
        "\n",
        "Instructions:\n",
        "- Questions should sound like a real FL Studio user (e.g., \"How do I...\", \"What's this for...\", \"Can I use X to do Y?\").\n",
        "- Generate ONLY the questions.\n",
        "- Base questions strictly on the provided context. No outside knowledge.\n",
        "- Ensure questions are diverse and distinct.\n",
        "- Avoid explicitly mentioning \"FL Studio\" in the questions, as the context is already assumed. For example, instead of \"How do I do X in FL Studio?\", ask \"How do I do X?\".\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Zg0gt_7Oufth"
      },
      "id": "Zg0gt_7Oufth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef43fe59-a29c-481b-b086-e98e55016d3e",
      "metadata": {
        "id": "ef43fe59-a29c-481b-b086-e98e55016d3e"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "train_dataset = generate_qa_embedding_pairs(\n",
        "    llm=OpenAI(model=\"gpt-4.1-mini-2025-04-14\"),\n",
        "    nodes=train_nodes,\n",
        "    qa_generate_prompt_tmpl=CUSTOM_PROMPT,\n",
        "    output_path=\"train_dataset.json\",\n",
        ")\n",
        "val_dataset = generate_qa_embedding_pairs(\n",
        "    llm=OpenAI(model=\"gpt-4.1-mini-2025-04-14\"),\n",
        "    nodes=val_nodes,\n",
        "    qa_generate_prompt_tmpl=CUSTOM_PROMPT,\n",
        "    output_path=\"val_dataset.json\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743f163c-25df-4c18-9abe-05052b034d70",
      "metadata": {
        "id": "743f163c-25df-4c18-9abe-05052b034d70"
      },
      "outputs": [],
      "source": [
        "# [Optional] Load\n",
        "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
        "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62368cb8-a303-48b1-8429-5e3655abcc3b",
      "metadata": {
        "id": "62368cb8-a303-48b1-8429-5e3655abcc3b"
      },
      "source": [
        "## Run Embedding Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0",
      "metadata": {
        "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0"
      },
      "outputs": [],
      "source": [
        "from llama_index.finetuning import SentenceTransformersFinetuneEngine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.finetuning.embeddings.sentence_transformer import SentenceTransformersFinetuneEngine\n",
        "\n",
        "def patched_finetune(self, **train_kwargs):\n",
        "    \"\"\"Monkey-patched finetune method without unsupported arguments.\"\"\"\n",
        "    self.model.fit(\n",
        "        train_objectives=[(self.loader, self.loss)],\n",
        "        epochs=self.epochs,\n",
        "        warmup_steps=self.warmup_steps,\n",
        "        output_path=self.model_output_path,\n",
        "        show_progress_bar=self.show_progress_bar,\n",
        "        evaluator=self.evaluator,\n",
        "        evaluation_steps=self.evaluation_steps,\n",
        "        checkpoint_path=self.checkpoint_path,\n",
        "        checkpoint_save_steps=self.checkpoint_save_steps,\n",
        "        checkpoint_save_total_limit=self.checkpoint_save_total_limit,\n",
        "        # 🚫 Removed: resume_from_checkpoint\n",
        "    )\n",
        "\n",
        "# Replace the original method with the patched one\n",
        "SentenceTransformersFinetuneEngine.finetune = patched_finetune\n"
      ],
      "metadata": {
        "id": "gqmCU2y6uwNl"
      },
      "id": "gqmCU2y6uwNl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
      "metadata": {
        "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc"
      },
      "outputs": [],
      "source": [
        "finetune_engine = SentenceTransformersFinetuneEngine(\n",
        "    train_dataset,\n",
        "    trust_remote_code=True,\n",
        "    model_id=\"Alibaba-NLP/gte-multilingual-base\",\n",
        "    model_output_path=\"test_model\",\n",
        "    val_dataset=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
      "metadata": {
        "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b"
      },
      "outputs": [],
      "source": [
        "finetune_engine.finetune()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493",
      "metadata": {
        "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493"
      },
      "outputs": [],
      "source": [
        "embed_model = finetune_engine.get_finetuned_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d16ec01-c29d-4742-aa3c-5ded6ae7c5a7",
      "metadata": {
        "id": "5d16ec01-c29d-4742-aa3c-5ded6ae7c5a7"
      },
      "outputs": [],
      "source": [
        "embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "828dd6fe-9a8a-419b-8663-56d81ce73774",
      "metadata": {
        "id": "828dd6fe-9a8a-419b-8663-56d81ce73774"
      },
      "source": [
        "## Evaluate Finetuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a66b83-4cbb-4374-a632-0f1bb2b785ab",
      "metadata": {
        "id": "f4a66b83-4cbb-4374-a632-0f1bb2b785ab"
      },
      "source": [
        "In this section, we evaluate 3 different embedding models:\n",
        "1. proprietary OpenAI embedding,\n",
        "2. open source `BAAI/bge-small-en`, and\n",
        "3. our finetuned embedding model.\n",
        "\n",
        "We consider 2 evaluation approaches:\n",
        "1. a simple custom **hit rate** metric\n",
        "2. using `InformationRetrievalEvaluator` from sentence_transformers\n",
        "\n",
        "We show that finetuning on synthetic (LLM-generated) dataset significantly improve upon an opensource embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d5176f-1f21-4bcb-adf5-da1c4cccb8d3",
      "metadata": {
        "id": "57d5176f-1f21-4bcb-adf5-da1c4cccb8d3"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.schema import TextNode\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda4c2b8-1ad8-420c-83d2-b88e0519895d",
      "metadata": {
        "id": "dda4c2b8-1ad8-420c-83d2-b88e0519895d"
      },
      "source": [
        "### Define eval function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398c24d3-3d72-4ce8-94a4-2da9c1b2605c",
      "metadata": {
        "id": "398c24d3-3d72-4ce8-94a4-2da9c1b2605c"
      },
      "source": [
        "**Option 1**: We use a simple **hit rate** metric for evaluation:\n",
        "* for each (query, relevant_doc) pair,\n",
        "* we retrieve top-k documents with the query,  and\n",
        "* it's a **hit** if the results contain the relevant_doc.\n",
        "\n",
        "This approach is very simple and intuitive, and we can apply it to both the proprietary OpenAI embedding as well as our open source and fine-tuned embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89401d3-a157-4f96-86d4-212e631a54bc",
      "metadata": {
        "id": "b89401d3-a157-4f96-86d4-212e631a54bc"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "    corpus = dataset.corpus\n",
        "    queries = dataset.queries\n",
        "    relevant_docs = dataset.relevant_docs\n",
        "\n",
        "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
        "    index = VectorStoreIndex(\n",
        "        nodes, embed_model=embed_model, show_progress=True\n",
        "    )\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    eval_results = []\n",
        "    for query_id, query in tqdm(queries.items()):\n",
        "        retrieved_nodes = retriever.retrieve(query)\n",
        "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
        "        expected_id = relevant_docs[query_id][0]\n",
        "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
        "\n",
        "        eval_result = {\n",
        "            \"is_hit\": is_hit,\n",
        "            \"retrieved\": retrieved_ids,\n",
        "            \"expected\": expected_id,\n",
        "            \"query\": query_id,\n",
        "        }\n",
        "        eval_results.append(eval_result)\n",
        "    return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb16251-bb45-4de0-b65a-e15aa76e0f1e",
      "metadata": {
        "id": "7eb16251-bb45-4de0-b65a-e15aa76e0f1e"
      },
      "source": [
        "**Option 2**: We use the `InformationRetrievalEvaluator` from sentence_transformers.\n",
        "\n",
        "This provides a more comprehensive suite of metrics, but we can only run it against the sentencetransformers compatible models (open source and our finetuned model, *not* the OpenAI embedding model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e89702-ea35-4c22-99c7-f89a5428ef95",
      "metadata": {
        "id": "88e89702-ea35-4c22-99c7-f89a5428ef95"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def evaluate_st(\n",
        "    dataset,\n",
        "    model_id,\n",
        "    name,\n",
        "):\n",
        "    corpus = dataset.corpus\n",
        "    queries = dataset.queries\n",
        "    relevant_docs = dataset.relevant_docs\n",
        "\n",
        "    evaluator = InformationRetrievalEvaluator(\n",
        "        queries, corpus, relevant_docs, name=name\n",
        "    )\n",
        "    model = SentenceTransformer(model_id)\n",
        "    output_path = \"results/\"\n",
        "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
        "    return evaluator(model, output_path=output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2d33dd-c39f-4c05-8adc-65db12163c88",
      "metadata": {
        "id": "af2d33dd-c39f-4c05-8adc-65db12163c88"
      },
      "source": [
        "### Run Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c630aa25-2395-4a8b-83cf-2885fbc862f4",
      "metadata": {
        "id": "c630aa25-2395-4a8b-83cf-2885fbc862f4"
      },
      "source": [
        "#### OpenAI\n",
        "\n",
        "Note: this might take a few minutes to run since we have to embed the corpus and queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a0784f-415e-4d3a-8c88-757b28b9e5df",
      "metadata": {
        "id": "61a0784f-415e-4d3a-8c88-757b28b9e5df"
      },
      "outputs": [],
      "source": [
        "ada = OpenAIEmbedding()\n",
        "ada_val_results = evaluate(val_dataset, ada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc73212-fc53-48c1-b347-f5ee3a29ae82",
      "metadata": {
        "id": "ccc73212-fc53-48c1-b347-f5ee3a29ae82"
      },
      "outputs": [],
      "source": [
        "df_ada = pd.DataFrame(ada_val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25eb61bb-c287-40fe-b3c7-bbfc2d2b1b94",
      "metadata": {
        "id": "25eb61bb-c287-40fe-b3c7-bbfc2d2b1b94"
      },
      "outputs": [],
      "source": [
        "hit_rate_ada = df_ada[\"is_hit\"].mean()\n",
        "hit_rate_ada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1bd6c62-65a8-4f72-a67c-d0d62c92d7d1",
      "metadata": {
        "id": "a1bd6c62-65a8-4f72-a67c-d0d62c92d7d1"
      },
      "source": [
        "### BAAI/bge-small-en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24454aeb-9e3e-4954-ab70-647102ed7f82",
      "metadata": {
        "id": "24454aeb-9e3e-4954-ab70-647102ed7f82"
      },
      "outputs": [],
      "source": [
        "bge = \"local:BAAI/bge-small-en\"\n",
        "bge_val_results = evaluate(val_dataset, bge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da27e48-1c90-4994-aac4-96b5b1638647",
      "metadata": {
        "id": "2da27e48-1c90-4994-aac4-96b5b1638647"
      },
      "outputs": [],
      "source": [
        "df_bge = pd.DataFrame(bge_val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddc4fe0-b240-4c15-9b2d-a4c79f9aaac2",
      "metadata": {
        "id": "3ddc4fe0-b240-4c15-9b2d-a4c79f9aaac2"
      },
      "outputs": [],
      "source": [
        "hit_rate_bge = df_bge[\"is_hit\"].mean()\n",
        "hit_rate_bge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c16df14-6564-41ec-8816-348675bb0fd4",
      "metadata": {
        "id": "2c16df14-6564-41ec-8816-348675bb0fd4"
      },
      "outputs": [],
      "source": [
        "evaluate_st(val_dataset, \"BAAI/bge-small-en\", name=\"bge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd87550-f547-4b8b-b21a-f72b355e2cd7",
      "metadata": {
        "id": "1fd87550-f547-4b8b-b21a-f72b355e2cd7"
      },
      "source": [
        "### Finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402dd440-1934-4778-8ff5-28e15cf1f2d3",
      "metadata": {
        "id": "402dd440-1934-4778-8ff5-28e15cf1f2d3"
      },
      "outputs": [],
      "source": [
        "finetuned = \"local:test_model\"\n",
        "val_results_finetuned = evaluate(val_dataset, finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd24643-17cb-4773-a535-77f3f8fa2d48",
      "metadata": {
        "id": "ffd24643-17cb-4773-a535-77f3f8fa2d48"
      },
      "outputs": [],
      "source": [
        "df_finetuned = pd.DataFrame(val_results_finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1dccd1-bbd4-427f-a520-b1011643d83b",
      "metadata": {
        "id": "ec1dccd1-bbd4-427f-a520-b1011643d83b"
      },
      "outputs": [],
      "source": [
        "hit_rate_finetuned = df_finetuned[\"is_hit\"].mean()\n",
        "hit_rate_finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8dd38e-f13d-43e1-9802-cc94b854526b",
      "metadata": {
        "id": "9d8dd38e-f13d-43e1-9802-cc94b854526b"
      },
      "outputs": [],
      "source": [
        "evaluate_st(val_dataset, \"test_model\", name=\"finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc290bc-5cc3-4ee4-b8ab-e68371441643",
      "metadata": {
        "id": "fbc290bc-5cc3-4ee4-b8ab-e68371441643"
      },
      "source": [
        "### Summary of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f906a11-6a95-4f10-9069-140bf5a56246",
      "metadata": {
        "id": "6f906a11-6a95-4f10-9069-140bf5a56246"
      },
      "source": [
        "#### Hit rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705fbe3c-2843-4bab-bb5c-16027fc5564b",
      "metadata": {
        "id": "705fbe3c-2843-4bab-bb5c-16027fc5564b"
      },
      "outputs": [],
      "source": [
        "df_ada[\"model\"] = \"ada\"\n",
        "df_bge[\"model\"] = \"bge\"\n",
        "df_finetuned[\"model\"] = \"fine_tuned\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebc363c-cd07-4dab-916e-1618d16d1254",
      "metadata": {
        "id": "bebc363c-cd07-4dab-916e-1618d16d1254"
      },
      "source": [
        "We can see that fine-tuning our small open-source embedding model drastically improve its retrieval quality (even approaching the quality of the proprietary OpenAI embedding)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f38b4b-1b40-42da-a054-ea9593d3e602",
      "metadata": {
        "id": "57f38b4b-1b40-42da-a054-ea9593d3e602"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat([df_ada, df_bge, df_finetuned])\n",
        "df_all.groupby(\"model\").mean(\"is_hit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08094d07-2c0a-44ca-ad2f-8d8bf1387ed9",
      "metadata": {
        "id": "08094d07-2c0a-44ca-ad2f-8d8bf1387ed9"
      },
      "source": [
        "#### InformationRetrievalEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d0444e-a824-42d6-9ddb-4da7179902bc",
      "metadata": {
        "id": "27d0444e-a824-42d6-9ddb-4da7179902bc"
      },
      "outputs": [],
      "source": [
        "df_st_bge = pd.read_csv(\n",
        "    \"results/Information-Retrieval_evaluation_bge_results.csv\"\n",
        ")\n",
        "df_st_finetuned = pd.read_csv(\n",
        "    \"results/Information-Retrieval_evaluation_finetuned_results.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0903ed3-df05-4d98-8b0a-6f352c681735",
      "metadata": {
        "id": "c0903ed3-df05-4d98-8b0a-6f352c681735"
      },
      "source": [
        "We can see that embedding finetuning improves metrics consistently across the suite of eval metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ec1c46-5aa0-4f8a-a0c5-2553e08cceb1",
      "metadata": {
        "id": "81ec1c46-5aa0-4f8a-a0c5-2553e08cceb1"
      },
      "outputs": [],
      "source": [
        "df_st_bge[\"model\"] = \"bge\"\n",
        "df_st_finetuned[\"model\"] = \"fine_tuned\"\n",
        "df_st_all = pd.concat([df_st_bge, df_st_finetuned])\n",
        "df_st_all = df_st_all.set_index(\"model\")\n",
        "df_st_all"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}